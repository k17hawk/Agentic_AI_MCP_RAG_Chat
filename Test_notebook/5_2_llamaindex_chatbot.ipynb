{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02708bff-4793-4be3-be12-dab15fa571bc",
   "metadata": {},
   "source": [
    "**5.1. Building a chatbot with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227bf712-6627-4bfe-8456-619a4dbca712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q llama-index llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9034df4-de52-41b7-b7a0-6c2f76feb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3058a56-ea82-46b5-9c10-4cb0fddcfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68733237-7ba7-4c85-9d2a-95c0b499cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c289d9-3b23-4c7d-bec4-ba06c3eb522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    history = [\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=\"You are a helpful assistant. Be concise and accurate.\")\n",
    "    ]\n",
    "\n",
    "    print(\"LlamaIndex Chatbot. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "\n",
    "        # add user message\n",
    "        history.append(ChatMessage(role=MessageRole.USER, content=user_input))\n",
    "\n",
    "        # call LLM with full history\n",
    "        resp = llm.chat(messages=history)  # ChatResponse\n",
    "        answer = resp.message.content\n",
    "\n",
    "        print(f\"Bot: {answer}\\n\")\n",
    "\n",
    "        # add assistant message\n",
    "        history.append(ChatMessage(role=MessageRole.ASSISTANT, content=answer))\n",
    "\n",
    "        print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9f72fb-4ee5-4e08-b69f-94802f33332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex Chatbot. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is Deep Learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Deep Learning is a subset of Machine Learning that uses neural networks with multiple layers to analyze and interpret data. It's inspired by the human brain's structure and function, and is particularly useful for tasks like image recognition, speech recognition, and natural language processing.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Give some applications.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Some applications of Deep Learning include:\n",
      "\n",
      "1. **Image Recognition**: Self-driving cars, facial recognition, object detection.\n",
      "2. **Speech Recognition**: Virtual assistants (e.g. Siri, Alexa), voice-to-text systems.\n",
      "3. **Natural Language Processing**: Language translation, text summarization, sentiment analysis.\n",
      "4. **Medical Diagnosis**: Disease detection, medical image analysis, personalized medicine.\n",
      "5. **Autonomous Systems**: Robotics, drones, autonomous vehicles.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f682c62-6acf-4209-b210-6b167bb0bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building anchors for 3 users...\n",
      "Created 6 total anchors\n",
      "Average anchors per user: 2.00\n",
      "\n",
      "Ranking Context for user1:\n",
      "Mode: global_popular\n",
      "Signal Strength: no_history\n",
      "Number of Active Anchors: 0\n",
      "\n",
      "System Statistics:\n",
      "  total_users: 3\n",
      "  total_anchors: 6\n",
      "  avg_anchors_per_user: 2.0\n",
      "  users_with_single_anchor: 1\n",
      "  users_with_multiple_anchors: 2\n",
      "  avg_visits_per_anchor: 1.0\n",
      "  avg_spatial_std_km: 0.0\n",
      "  max_spatial_std_km: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Temporary Location Anchor Manager for Restaurant Ranking\n",
    "Handles cold start, multi-location users, and privacy-preserving location inference\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class LocationAnchor:\n",
    "    \"\"\"Represents a temporary location anchor for a user\"\"\"\n",
    "    anchor_id: str\n",
    "    user_id: str\n",
    "    centroid_lat: float\n",
    "    centroid_lon: float\n",
    "    earliest_visit: datetime\n",
    "    latest_visit: datetime\n",
    "    visit_count: int\n",
    "    spatial_std: float  # Standard deviation of locations (if merged)\n",
    "    source_visits: List[Tuple[str, float, float, datetime]]  # (business_id, lat, lon, timestamp)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'anchor_id': self.anchor_id,\n",
    "            'user_id': self.user_id,\n",
    "            'centroid_lat': self.centroid_lat,\n",
    "            'centroid_lon': self.centroid_lon,\n",
    "            'earliest_visit': self.earliest_visit.isoformat(),\n",
    "            'latest_visit': self.latest_visit.isoformat(),\n",
    "            'visit_count': self.visit_count,\n",
    "            'spatial_std': self.spatial_std,\n",
    "            'source_visits': [(bid, lat, lon, ts.isoformat()) \n",
    "                             for bid, lat, lon, ts in self.source_visits]\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data):\n",
    "        data['earliest_visit'] = datetime.fromisoformat(data['earliest_visit'])\n",
    "        data['latest_visit'] = datetime.fromisoformat(data['latest_visit'])\n",
    "        data['source_visits'] = [(bid, lat, lon, datetime.fromisoformat(ts)) \n",
    "                                 for bid, lat, lon, ts in data['source_visits']]\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RankingContext:\n",
    "    \"\"\"Context for restaurant ranking\"\"\"\n",
    "    mode: str  # 'global_popular', 'local_explore', 'personalized'\n",
    "    signal_strength: str  # 'no_history', 'single_visit', 'established'\n",
    "    anchors: List[Tuple[LocationAnchor, float]]  # (anchor, strength_score)\n",
    "    primary_radius_km: float\n",
    "    extended_radius_km: float\n",
    "    min_personalization_score: float  # For extended radius\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate distance between two points on Earth in kilometers\n",
    "    \"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    \n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    delta_lat = np.radians(lat2 - lat1)\n",
    "    delta_lon = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(delta_lat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(delta_lon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "\n",
    "def calculate_spatial_std(locations: List[Tuple[float, float]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate spatial standard deviation of a cluster of locations\n",
    "    \"\"\"\n",
    "    if len(locations) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    lats = [loc[0] for loc in locations]\n",
    "    lons = [loc[1] for loc in locations]\n",
    "    \n",
    "    # Convert to km using approximation\n",
    "    lat_std = np.std(lats) * 111  # 1 degree lat â‰ˆ 111 km\n",
    "    lon_std = np.std(lons) * 111 * np.cos(np.radians(np.mean(lats)))\n",
    "    \n",
    "    return np.sqrt(lat_std**2 + lon_std**2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANCHOR STRENGTH CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_anchor_strength(\n",
    "    days_since_last_visit: float,\n",
    "    visit_count: int,\n",
    "    spatial_tightness: float,\n",
    "    half_life_days: float = 90.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate strength score for an anchor\n",
    "    \n",
    "    Args:\n",
    "        days_since_last_visit: Days since the most recent visit\n",
    "        visit_count: Number of visits contributing to this anchor\n",
    "        spatial_tightness: Spatial std dev in km (lower = tighter cluster)\n",
    "        half_life_days: Time for anchor to decay to 50% strength\n",
    "    \n",
    "    Returns:\n",
    "        Strength score (0 to ~10, higher = stronger)\n",
    "    \"\"\"\n",
    "    # Exponential time decay\n",
    "    time_weight = np.exp(-days_since_last_visit / half_life_days)\n",
    "    \n",
    "    # Visit frequency boost (log scale to prevent outliers)\n",
    "    frequency_weight = np.log1p(visit_count)\n",
    "    \n",
    "    # Spatial confidence (penalize loose clusters)\n",
    "    spatial_weight = 1.0 / (1.0 + spatial_tightness)\n",
    "    \n",
    "    return time_weight * frequency_weight * spatial_weight\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANCHOR MERGING LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "def should_merge_anchors(\n",
    "    anchor1: LocationAnchor,\n",
    "    anchor2: LocationAnchor,\n",
    "    max_distance_km: float = 50.0,\n",
    "    min_time_spread_days: int = 7,\n",
    "    min_total_visits: int = 3\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine if two anchors should be merged\n",
    "    \n",
    "    Criteria:\n",
    "    1. Geographic proximity (within max_distance_km)\n",
    "    2. Temporal spread (not same-day burst)\n",
    "    3. Minimum visit count (meaningful pattern)\n",
    "    \"\"\"\n",
    "    # Calculate distance between centroids\n",
    "    distance = haversine_distance(\n",
    "        anchor1.centroid_lat, anchor1.centroid_lon,\n",
    "        anchor2.centroid_lat, anchor2.centroid_lon\n",
    "    )\n",
    "    \n",
    "    if distance > max_distance_km:\n",
    "        return False\n",
    "    \n",
    "    # Check temporal spread\n",
    "    all_visits = sorted([v[3] for v in anchor1.source_visits + anchor2.source_visits])\n",
    "    time_spread = (all_visits[-1] - all_visits[0]).days\n",
    "    \n",
    "    if time_spread < min_time_spread_days:\n",
    "        return False\n",
    "    \n",
    "    # Check total visits\n",
    "    total_visits = anchor1.visit_count + anchor2.visit_count\n",
    "    \n",
    "    if total_visits < min_total_visits:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def merge_anchors(anchor1: LocationAnchor, anchor2: LocationAnchor) -> LocationAnchor:\n",
    "    \"\"\"\n",
    "    Merge two anchors into a single anchor\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    \n",
    "    combined_visits = anchor1.source_visits + anchor2.source_visits\n",
    "    \n",
    "    # Calculate new centroid (weighted by visit count per location)\n",
    "    all_lats = [v[1] for v in combined_visits]\n",
    "    all_lons = [v[2] for v in combined_visits]\n",
    "    \n",
    "    new_centroid_lat = np.mean(all_lats)\n",
    "    new_centroid_lon = np.mean(all_lons)\n",
    "    \n",
    "    # Calculate spatial std\n",
    "    locations = [(v[1], v[2]) for v in combined_visits]\n",
    "    spatial_std = calculate_spatial_std(locations)\n",
    "    \n",
    "    # Time boundaries\n",
    "    all_timestamps = [v[3] for v in combined_visits]\n",
    "    earliest = min(all_timestamps)\n",
    "    latest = max(all_timestamps)\n",
    "    \n",
    "    return LocationAnchor(\n",
    "        anchor_id=str(uuid.uuid4()),\n",
    "        user_id=anchor1.user_id,\n",
    "        centroid_lat=new_centroid_lat,\n",
    "        centroid_lon=new_centroid_lon,\n",
    "        earliest_visit=earliest,\n",
    "        latest_visit=latest,\n",
    "        visit_count=len(combined_visits),\n",
    "        spatial_std=spatial_std,\n",
    "        source_visits=combined_visits\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANCHOR MANAGER - MAIN CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class LocationAnchorManager:\n",
    "    \"\"\"\n",
    "    Main class for managing user location anchors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        merging_distance_km: float = 50.0,\n",
    "        primary_radius_km: float = 40.0,\n",
    "        extended_radius_km: float = 100.0,\n",
    "        anchor_max_age_days: int = 365,\n",
    "        min_anchor_strength: float = 0.1\n",
    "    ):\n",
    "        self.merging_distance_km = merging_distance_km\n",
    "        self.primary_radius_km = primary_radius_km\n",
    "        self.extended_radius_km = extended_radius_km\n",
    "        self.anchor_max_age_days = anchor_max_age_days\n",
    "        self.min_anchor_strength = min_anchor_strength\n",
    "        \n",
    "        # Storage: user_id -> List[LocationAnchor]\n",
    "        self.user_anchors: Dict[str, List[LocationAnchor]] = defaultdict(list)\n",
    "    \n",
    "    def build_anchors_from_dataframe(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build anchors from tips/reviews dataframe\n",
    "        \n",
    "        Expected columns:\n",
    "        - user_id\n",
    "        - business_id\n",
    "        - latitude\n",
    "        - longitude\n",
    "        - tip_date or reviews_date\n",
    "        \"\"\"\n",
    "        import uuid\n",
    "        \n",
    "        # Determine date column\n",
    "        date_col = 'tip_date' if 'tip_date' in df.columns else 'reviews_date'\n",
    "        \n",
    "        # Sort by user and date\n",
    "        df_sorted = df.sort_values(['user_id', date_col]).copy()\n",
    "        \n",
    "        print(f\"Building anchors for {df_sorted['user_id'].nunique()} users...\")\n",
    "        \n",
    "        for user_id, user_df in df_sorted.groupby('user_id'):\n",
    "            user_anchors = []\n",
    "            \n",
    "            for _, row in user_df.iterrows():\n",
    "                # Create initial anchor from each visit\n",
    "                anchor = LocationAnchor(\n",
    "                    anchor_id=str(uuid.uuid4()),\n",
    "                    user_id=user_id,\n",
    "                    centroid_lat=row['latitude'],\n",
    "                    centroid_lon=row['longitude'],\n",
    "                    earliest_visit=pd.to_datetime(row[date_col]),\n",
    "                    latest_visit=pd.to_datetime(row[date_col]),\n",
    "                    visit_count=1,\n",
    "                    spatial_std=0.0,\n",
    "                    source_visits=[(\n",
    "                        row['business_id'],\n",
    "                        row['latitude'],\n",
    "                        row['longitude'],\n",
    "                        pd.to_datetime(row[date_col])\n",
    "                    )]\n",
    "                )\n",
    "                user_anchors.append(anchor)\n",
    "            \n",
    "            # Attempt to merge nearby anchors\n",
    "            merged_anchors = self._merge_anchor_list(user_anchors)\n",
    "            self.user_anchors[user_id] = merged_anchors\n",
    "        \n",
    "        print(f\"Created {sum(len(anchors) for anchors in self.user_anchors.values())} total anchors\")\n",
    "        print(f\"Average anchors per user: {np.mean([len(a) for a in self.user_anchors.values()]):.2f}\")\n",
    "    \n",
    "    def _merge_anchor_list(self, anchors: List[LocationAnchor]) -> List[LocationAnchor]:\n",
    "        \"\"\"\n",
    "        Iteratively merge anchors that meet merging criteria\n",
    "        \"\"\"\n",
    "        if len(anchors) <= 1:\n",
    "            return anchors\n",
    "        \n",
    "        merged = True\n",
    "        current_anchors = anchors.copy()\n",
    "        \n",
    "        while merged:\n",
    "            merged = False\n",
    "            new_anchors = []\n",
    "            used_indices = set()\n",
    "            \n",
    "            for i in range(len(current_anchors)):\n",
    "                if i in used_indices:\n",
    "                    continue\n",
    "                \n",
    "                merged_with_any = False\n",
    "                for j in range(i + 1, len(current_anchors)):\n",
    "                    if j in used_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    if should_merge_anchors(\n",
    "                        current_anchors[i],\n",
    "                        current_anchors[j],\n",
    "                        self.merging_distance_km\n",
    "                    ):\n",
    "                        # Merge i and j\n",
    "                        merged_anchor = merge_anchors(current_anchors[i], current_anchors[j])\n",
    "                        new_anchors.append(merged_anchor)\n",
    "                        used_indices.add(i)\n",
    "                        used_indices.add(j)\n",
    "                        merged_with_any = True\n",
    "                        merged = True\n",
    "                        break\n",
    "                \n",
    "                if not merged_with_any:\n",
    "                    new_anchors.append(current_anchors[i])\n",
    "                    used_indices.add(i)\n",
    "            \n",
    "            current_anchors = new_anchors\n",
    "        \n",
    "        return current_anchors\n",
    "    \n",
    "    def get_active_anchors(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        current_time: datetime\n",
    "    ) -> List[Tuple[LocationAnchor, float]]:\n",
    "        \"\"\"\n",
    "        Get active anchors with strength scores for a user\n",
    "        \n",
    "        Returns:\n",
    "            List of (anchor, strength_score) tuples, sorted by strength (descending)\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_anchors:\n",
    "            return []\n",
    "        \n",
    "        active_anchors = []\n",
    "        \n",
    "        for anchor in self.user_anchors[user_id]:\n",
    "            days_since_visit = (current_time - anchor.latest_visit).days\n",
    "            \n",
    "            # Filter out very old anchors\n",
    "            if days_since_visit > self.anchor_max_age_days:\n",
    "                continue\n",
    "            \n",
    "            # Calculate strength\n",
    "            strength = calculate_anchor_strength(\n",
    "                days_since_last_visit=days_since_visit,\n",
    "                visit_count=anchor.visit_count,\n",
    "                spatial_tightness=anchor.spatial_std\n",
    "            )\n",
    "            \n",
    "            # Only include if above minimum threshold\n",
    "            if strength >= self.min_anchor_strength:\n",
    "                active_anchors.append((anchor, strength))\n",
    "        \n",
    "        # Sort by strength (descending)\n",
    "        active_anchors.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top 3 strongest anchors\n",
    "        return active_anchors[:3]\n",
    "    \n",
    "    def get_ranking_context(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        current_time: datetime\n",
    "    ) -> RankingContext:\n",
    "        \"\"\"\n",
    "        Get ranking context for restaurant recommendation\n",
    "        \n",
    "        This determines:\n",
    "        - What mode to use (global/local/personalized)\n",
    "        - What anchors to use\n",
    "        - What radius to search\n",
    "        \"\"\"\n",
    "        anchors = self.get_active_anchors(user_id, current_time)\n",
    "        \n",
    "        if len(anchors) == 0:\n",
    "            # TRUE cold start - no location anchors yet\n",
    "            return RankingContext(\n",
    "                mode='global_popular',\n",
    "                signal_strength='no_history',\n",
    "                anchors=[],\n",
    "                primary_radius_km=0,\n",
    "                extended_radius_km=0,\n",
    "                min_personalization_score=0.0\n",
    "            )\n",
    "        \n",
    "        elif len(anchors) == 1 and anchors[0][0].visit_count == 1:\n",
    "            # Weak signal - single visit only\n",
    "            return RankingContext(\n",
    "                mode='local_explore',\n",
    "                signal_strength='single_visit',\n",
    "                anchors=anchors,\n",
    "                primary_radius_km=self.primary_radius_km * 1.5,  # Wider search\n",
    "                extended_radius_km=self.extended_radius_km,\n",
    "                min_personalization_score=0.85\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Strong signal - multiple visits or merged anchors\n",
    "            return RankingContext(\n",
    "                mode='personalized',\n",
    "                signal_strength='established',\n",
    "                anchors=anchors,\n",
    "                primary_radius_km=self.primary_radius_km,\n",
    "                extended_radius_km=self.extended_radius_km,\n",
    "                min_personalization_score=0.90\n",
    "            )\n",
    "    \n",
    "    def get_candidate_restaurants(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        all_restaurants: pd.DataFrame,\n",
    "        current_time: datetime,\n",
    "        personalization_scores: Optional[Dict[str, float]] = None\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get candidate restaurants for ranking\n",
    "        \n",
    "        Args:\n",
    "            user_id: User identifier\n",
    "            all_restaurants: DataFrame with columns [business_id, latitude, longitude, ...]\n",
    "            current_time: Current timestamp\n",
    "            personalization_scores: Optional dict of {business_id: personalization_score}\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with 'primary' and 'extended' candidate DataFrames\n",
    "        \"\"\"\n",
    "        context = self.get_ranking_context(user_id, current_time)\n",
    "        \n",
    "        if context.mode == 'global_popular':\n",
    "            # Cold start - return popular restaurants (no location filter)\n",
    "            return {\n",
    "                'primary': all_restaurants.copy(),\n",
    "                'extended': pd.DataFrame(),\n",
    "                'context': context\n",
    "            }\n",
    "        \n",
    "        # Filter restaurants by distance to anchors\n",
    "        primary_candidates = []\n",
    "        extended_candidates = []\n",
    "        \n",
    "        for _, restaurant in all_restaurants.iterrows():\n",
    "            rest_lat = restaurant['latitude']\n",
    "            rest_lon = restaurant['longitude']\n",
    "            rest_id = restaurant['business_id']\n",
    "            \n",
    "            # Find minimum distance to any anchor\n",
    "            min_distance = float('inf')\n",
    "            for anchor, strength in context.anchors:\n",
    "                dist = haversine_distance(\n",
    "                    rest_lat, rest_lon,\n",
    "                    anchor.centroid_lat, anchor.centroid_lon\n",
    "                )\n",
    "                min_distance = min(min_distance, dist)\n",
    "            \n",
    "            # Add distance column\n",
    "            restaurant_with_dist = restaurant.copy()\n",
    "            restaurant_with_dist['min_anchor_distance_km'] = min_distance\n",
    "            \n",
    "            # Classify as primary or extended\n",
    "            if min_distance <= context.primary_radius_km:\n",
    "                primary_candidates.append(restaurant_with_dist)\n",
    "            \n",
    "            elif min_distance <= context.extended_radius_km:\n",
    "                # Only include if personalization score is high enough\n",
    "                if personalization_scores is not None:\n",
    "                    pers_score = personalization_scores.get(rest_id, 0.0)\n",
    "                    if pers_score >= context.min_personalization_score:\n",
    "                        extended_candidates.append(restaurant_with_dist)\n",
    "                else:\n",
    "                    # Without personalization scores, include all in extended\n",
    "                    extended_candidates.append(restaurant_with_dist)\n",
    "        \n",
    "        return {\n",
    "            'primary': pd.DataFrame(primary_candidates),\n",
    "            'extended': pd.DataFrame(extended_candidates),\n",
    "            'context': context\n",
    "        }\n",
    "    \n",
    "    def save_anchors(self, filepath: str):\n",
    "        \"\"\"Save anchors to JSON file\"\"\"\n",
    "        data = {\n",
    "            user_id: [anchor.to_dict() for anchor in anchors]\n",
    "            for user_id, anchors in self.user_anchors.items()\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved anchors for {len(data)} users to {filepath}\")\n",
    "    \n",
    "    def load_anchors(self, filepath: str):\n",
    "        \"\"\"Load anchors from JSON file\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.user_anchors = {\n",
    "            user_id: [LocationAnchor.from_dict(a) for a in anchors]\n",
    "            for user_id, anchors in data.items()\n",
    "        }\n",
    "        \n",
    "        print(f\"Loaded anchors for {len(self.user_anchors)} users from {filepath}\")\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about the anchor system\"\"\"\n",
    "        all_anchors = [a for anchors in self.user_anchors.values() for a in anchors]\n",
    "        \n",
    "        if not all_anchors:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'total_users': len(self.user_anchors),\n",
    "            'total_anchors': len(all_anchors),\n",
    "            'avg_anchors_per_user': np.mean([len(a) for a in self.user_anchors.values()]),\n",
    "            'users_with_single_anchor': sum(1 for a in self.user_anchors.values() if len(a) == 1),\n",
    "            'users_with_multiple_anchors': sum(1 for a in self.user_anchors.values() if len(a) > 1),\n",
    "            'avg_visits_per_anchor': np.mean([a.visit_count for a in all_anchors]),\n",
    "            'avg_spatial_std_km': np.mean([a.spatial_std for a in all_anchors]),\n",
    "            'max_spatial_std_km': np.max([a.spatial_std for a in all_anchors]),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = pd.DataFrame({\n",
    "        'user_id': ['user1', 'user1', 'user1', 'user2', 'user2', 'user3'],\n",
    "        'business_id': ['rest1', 'rest2', 'rest3', 'rest4', 'rest5', 'rest6'],\n",
    "        'latitude': [40.7128, 40.7580, 40.7489, 43.6532, 43.6511, 34.0522],\n",
    "        'longitude': [-74.0060, -73.9855, -73.9680, -79.3832, -79.3470, -118.2437],\n",
    "        'tip_date': pd.to_datetime([\n",
    "            '2024-01-15', '2024-02-20', '2024-03-10',\n",
    "            '2024-01-10', '2024-06-15',\n",
    "            '2024-05-01'\n",
    "        ])\n",
    "    })\n",
    "    \n",
    "    # Initialize manager\n",
    "    manager = LocationAnchorManager(\n",
    "        merging_distance_km=50.0,\n",
    "        primary_radius_km=40.0,\n",
    "        extended_radius_km=100.0\n",
    "    )\n",
    "    \n",
    "    # Build anchors\n",
    "    manager.build_anchors_from_dataframe(sample_data)\n",
    "    \n",
    "    # Get ranking context for a user\n",
    "    current_time = datetime(2024, 12, 1)\n",
    "    context = manager.get_ranking_context('user1', current_time)\n",
    "    \n",
    "    print(f\"\\nRanking Context for user1:\")\n",
    "    print(f\"Mode: {context.mode}\")\n",
    "    print(f\"Signal Strength: {context.signal_strength}\")\n",
    "    print(f\"Number of Active Anchors: {len(context.anchors)}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    stats = manager.get_statistics()\n",
    "    print(f\"\\nSystem Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce60e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.DataFrame({\n",
    "        'user_id': ['user1', 'user1', 'user1', 'user2', 'user2', 'user3'],\n",
    "        'business_id': ['rest1', 'rest2', 'rest3', 'rest4', 'rest5', 'rest6'],\n",
    "        'latitude': [40.7128, 40.7580, 40.7489, 43.6532, 43.6511, 34.0522],\n",
    "        'longitude': [-74.0060, -73.9855, -73.9680, -79.3832, -79.3470, -118.2437],\n",
    "        'tip_date': pd.to_datetime([\n",
    "            '2024-01-15', '2024-02-20', '2024-03-10',\n",
    "            '2024-01-10', '2024-06-15',\n",
    "            '2024-05-01'\n",
    "        ])\n",
    "    })\n",
    "    \n",
    "    # Initialize manager\n",
    "manager = LocationAnchorManager(\n",
    "        merging_distance_km=50.0,\n",
    "        primary_radius_km=40.0,\n",
    "        extended_radius_km=100.0\n",
    "    )\n",
    "    \n",
    "    # Build anchors\n",
    "    #     \n",
    "manager.build_anchors_from_dataframe(sample_data)\n",
    "    \n",
    "    # Get ranking context for a user\n",
    "current_time = datetime(2024, 12, 1)\n",
    "context = manager.get_ranking_context('user1', current_time)\n",
    "    \n",
    "print(f\"\\nRanking Context for user1:\")\n",
    "print(f\"Mode: {context.mode}\")\n",
    "print(f\"Signal Strength: {context.signal_strength}\")\n",
    "print(f\"Number of Active Anchors: {len(context.anchors)}\")\n",
    "    \n",
    "    # Show statistics\n",
    "stats = manager.get_statistics()\n",
    "print(f\"\\nSystem Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e67a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
